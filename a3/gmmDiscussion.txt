The classification rate through GMM is very high, almost 96% to 100%, expecially when M and maxIter are higher. However, we can still see some difference on log likelihood. As the M and maxIter increase, the classification will also increase. It's clear that when M=1, the log likelihood would be 219123973 and when M=8, llk drops to 69963760. However, the training set converges very fast such that from the second iteration, it's almost convergent.

So theoretically, if we add more Gaussian Mixture Models in calculation, it would improves the accuracy. It's possible to have a separate GMM for each class. Another way is to preprocess the data to denoise and normalize it. The last but not the least, there is a paper Improving Accuracy of Gaussian Mixure Model Classifiers with Additional Discriminative Training by Ajay M. Patrikar and John. P. Baker(2016). They applied the Moore Penrose pseudi-inverse method to a GMM previously trained with the EM algorithm. They declaired the performance of the GMM would be improved significantly.

As this speaker is not trained, the model doesn't know there is such a tester. Thus, it will still try to find the one with the closest voice. It's impossible to identify who the person is without the training value, however, it's possible to see that the person is not trained when the likelihood is too far from all of trained model. When 

It might be possible to use neural network to identify the person. It's possible to build a CNN to learn the features directly from the spectrograms and then make a clustering. The features would be detected during hidden layers, filtered in pooling layer in the network and activated by a dence or softmax layer. Finally, run with the trained network we can estimate the speaker.
