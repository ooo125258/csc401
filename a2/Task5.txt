
Discussion :

{Enter your intriguing discussion (explaining observations/results) here}
This is the result for Task5. 

Answer the Question: The reference translations are not the same between the Hanzard one and the Google one. For example, "We are suggesting that we could pass a better bill" and "We believe it is possible to do better". They are telling different things. The translation of Google is something closer to our common life, when the Hanzard translation is closer to the status in court. Another example is this "That is true for every member of Parliament" in Hanzard and "This applies to all deputies." in Google. The one in Hanzard emphesis the identities of the member to be explicit and serious. However, This method is useless for most of the time for the public so the Google translation prefer to use a translation to be easier to understand, when the public often use it to represent the ones in their preferred translation.
It might be a better or worse choice, depending on the reference selected. If we find more better references, the length would be closer to the candidate thus the brevity would be more accurate. The precision would also be more accurate thus improve the accuracy of BLEU score. However, if we add more references, but the quality is not good, the reference will cover all of the correct and incorrect words and make a inflation on BLEU score. However, the accuracy of this model does not change. Then it would be a worse thing. Above all, quality is more important than quantity in references selection.


BLEU score analysis 
Generally the BLEU score would be increasing, as the increase of training set. The average of 25 BLEU scores are generally increasing when n is the same and there are more groups increased compared with the one decreased. As the size of the training set increase, the result should be more accurate. However, it's still clear that some of the BLEU scores are decreasing. One of the reason might be that the each english word has several meaning and each french word has several meaning. When the training set increases it makes the algorithm confuse but it can be solved by adding more training set. 
However, when the length of n-gram increases, the BLEU score would decrease. The n-gram calculate the existance of the continuous occurance of the translation. However, IBM1 is basically an point to point translation and as a result, it will almost keep the word order in source language. However, google translation and Hanzard will follow some kind of English grammar. Thus, the scores for 2-gram and 3-gram would be much smaller, even to zero. Another problem is that the accuracy for each word is not perfect. And when two words are combined together without correlation in meaning it makes a mess when the datasize is small(too many outliers). It would be remitted when the datasize is higher.

Tuning:
As the iteration increases, the change of BLEU score decreases. The difference between the 20 iterations and 50 iterations is very small and the time is acceptable. Thus I choose 20 as iteration numbers.


----------Evaluation START----------

### Evaluating AM model: 1000 ### 


BLEU scores with N-gram (n) = 1: 	0.4118	0.4444	0.6923	0.4286	0.5385	0.6000	0.3846	0.6250	0.3750	0.2983	0.5385	0.3846	0.5455	0.4444	0.4617	0.5367	0.5000	0.5789	0.2857	0.5429	0.6250	0.6364	0.4286	0.4737	0.3750
BLEU scores with N-gram (n) = 2: 	0.2269	0.2357	0.3397	0.1816	0.2996	0.4472	0.1790	0.4226	0.0000	0.0000	0.3669	0.2532	0.3303	0.2287	0.2569	0.4237	0.3333	0.4010	0.1482	0.4047	0.4226	0.5045	0.3145	0.2810	0.2315
BLEU scores with N-gram (n) = 3: 	0.0000	0.0000	0.0000	0.0000	0.0000	0.3684	0.0000	0.0000	0.0000	0.0000	0.2304	0.0000	0.0000	0.1484	0.0000	0.2547	0.2404	0.0000	0.0000	0.2646	0.0000	0.3047	0.2020	0.1668	0.0000


### Evaluating AM model: 10000 ### 


BLEU scores with N-gram (n) = 1: 	0.3529	0.4444	0.6154	0.5000	0.5385	0.6000	0.3846	0.7500	0.5000	0.3977	0.6154	0.3077	0.4545	0.5000	0.4617	0.5367	0.6000	0.6316	0.3571	0.5429	0.7500	0.7273	0.5000	0.4737	0.3750
BLEU scores with N-gram (n) = 2: 	0.1485	0.2357	0.4529	0.1961	0.2118	0.4472	0.1790	0.6547	0.0000	0.0000	0.3922	0.2265	0.2132	0.2970	0.3146	0.4237	0.4472	0.4189	0.1657	0.4047	0.5669	0.6030	0.3397	0.2810	0.2315
BLEU scores with N-gram (n) = 3: 	0.0000	0.0000	0.3341	0.0000	0.0000	0.3684	0.0000	0.5228	0.0000	0.0000	0.2409	0.0000	0.0000	0.1767	0.1976	0.2547	0.3684	0.0000	0.0000	0.2646	0.3770	0.4323	0.2126	0.1668	0.0000


### Evaluating AM model: 15000 ### 


BLEU scores with N-gram (n) = 1: 	0.4118	0.4444	0.5385	0.5000	0.5385	0.6000	0.4615	0.7500	0.3750	0.3977	0.6154	0.3077	0.5455	0.5000	0.4617	0.5367	0.6000	0.6316	0.3571	0.5429	0.6250	0.7273	0.5000	0.4211	0.3750
BLEU scores with N-gram (n) = 2: 	0.1604	0.2357	0.4237	0.1961	0.2118	0.4472	0.1961	0.6547	0.0000	0.0000	0.3922	0.2265	0.2335	0.2970	0.3146	0.4237	0.4472	0.4189	0.1657	0.4047	0.4226	0.6030	0.3397	0.2649	0.2315
BLEU scores with N-gram (n) = 3: 	0.0000	0.0000	0.3196	0.0000	0.0000	0.3684	0.0000	0.5228	0.0000	0.0000	0.2409	0.0000	0.0000	0.1767	0.1976	0.2547	0.3684	0.0000	0.0000	0.2646	0.0000	0.4323	0.2126	0.1604	0.0000


### Evaluating AM model: 30000 ### 


BLEU scores with N-gram (n) = 1: 	0.4706	0.4444	0.4615	0.5000	0.4615	0.6000	0.4615	0.6250	0.5000	0.3977	0.6154	0.2308	0.4545	0.5000	0.4617	0.4600	0.6000	0.5789	0.4286	0.3619	0.8750	0.6364	0.5000	0.4211	0.3750
BLEU scores with N-gram (n) = 2: 	0.1715	0.2357	0.2774	0.1961	0.1961	0.4472	0.1961	0.4226	0.0000	0.0000	0.3922	0.1387	0.2132	0.2970	0.3146	0.3397	0.4472	0.3587	0.2568	0.1908	0.7906	0.5045	0.3397	0.2649	0.2315
BLEU scores with N-gram (n) = 3: 	0.0000	0.0000	0.0000	0.0000	0.0000	0.3684	0.0000	0.3099	0.0000	0.0000	0.2409	0.0000	0.0000	0.1767	0.1976	0.2198	0.3684	0.0000	0.0000	0.0000	0.6786	0.3047	0.2126	0.1604	0.0000



----------Evaluation END----------
