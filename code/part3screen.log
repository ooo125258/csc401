b3175-03:~/csc401/code% python3 a1_classify.py -i out_mine.json.npz
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath_tests import inner1d
TODO Section 3.1
Method i
Method i
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
Method i
Method i
Method i
5
3.1 compare_values
[[1.00000000e+00 2.88500000e-01 3.14114682e-01 2.74319066e-01
  2.58511980e-01 2.78796772e-01 4.92832427e-01 7.07831325e-02
  2.02469136e-01 3.87755102e-01 9.97000000e+02 1.06000000e+02
  3.68000000e+02 5.52000000e+02 7.69000000e+02 1.41000000e+02
  4.18000000e+02 6.64000000e+02 7.43000000e+02 1.22000000e+02
  4.10000000e+02 7.50000000e+02 6.65000000e+02 1.45000000e+02
  3.90000000e+02 7.60000000e+02]
 [2.00000000e+00 2.43250000e-01 2.67015707e-01 2.50681199e-01
  2.34501348e-01 2.42005814e-01 5.04201681e-02 4.61847390e-02
  4.29629630e-02 8.49489796e-01 1.02000000e+02 1.01000000e+02
  9.20000000e+01 1.72800000e+03 7.30000000e+01 9.20000000e+01
  9.40000000e+01 1.73300000e+03 9.10000000e+01 9.30000000e+01
  8.70000000e+01 1.75400000e+03 1.16000000e+02 8.10000000e+01
  9.80000000e+01 1.66500000e+03]
 [3.00000000e+00 3.33625000e-01 3.83009709e-01 3.14475025e-01
  3.47022587e-01 3.07922770e-01 3.90014829e-01 3.09738956e-01
  1.66913580e-01 4.71938776e-01 7.89000000e+02 3.80000000e+02
  1.55000000e+02 6.99000000e+02 4.65000000e+02 6.17000000e+02
  2.10000000e+02 7.00000000e+02 4.71000000e+02 5.36000000e+02
  3.38000000e+02 6.80000000e+02 3.35000000e+02 4.29000000e+02
  2.71000000e+02 9.25000000e+02]
 [4.00000000e+00 3.13250000e-01 4.40781441e-01 2.81731910e-01
  3.28846154e-01 3.49030471e-01 1.78447850e-01 7.15361446e-01
  1.68888889e-01 1.92857143e-01 3.61000000e+02 1.25900000e+03
  1.72000000e+02 2.31000000e+02 1.40000000e+02 1.42500000e+03
  2.09000000e+02 2.18000000e+02 1.53000000e+02 1.27400000e+03
  3.42000000e+02 2.56000000e+02 1.65000000e+02 1.10000000e+03
  3.17000000e+02 3.78000000e+02]
 [5.00000000e+00 3.55125000e-01 3.92057457e-01 3.48602023e-01
  3.37685460e-01 3.34362594e-01 4.58724666e-01 2.94176707e-01
  2.80987654e-01 3.86734694e-01 9.28000000e+02 3.12000000e+02
  3.23000000e+02 4.60000000e+02 4.76000000e+02 5.86000000e+02
  3.93000000e+02 5.37000000e+02 5.23000000e+02 4.21000000e+02
  5.69000000e+02 5.12000000e+02 4.40000000e+02 3.62000000e+02
  4.00000000e+02 7.58000000e+02]]
TODO Section 3.2
a1 3.2
[0.29575, 0.3375, 0.355, 0.35775, 0.360375]
TODO Section 3.3
currently, the best classifier would be 5.
currently, 3.3 part1
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [12 13] are constant.
  UserWarning)
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide
  f = msb / msw
currently, 3.3 part1
currently, 3.3 part1
currently, 3.3 part1
currently, 3.3 part1
currently, 3.3 part1
3.3 The Features index for 1k:

[[103 140 149 159 171   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [ 15  64  82 103 118 119 140 149 159 171   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [ 11  15  29  48  64  82 103 112 118 119 122 133 140 146 148 149 153 159
  170 171   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [ 11  15  17  18  29  35  48  64  81  82  92  93 103 112 118 119 122 127
  133 136 140 144 146 148 149 153 154 159 170 171   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [ 11  15  17  18  19  22  27  29  35  36  41  48  64  81  82  92  93 103
  112 118 119 122 127 130 133 136 140 141 144 146 148 149 153 154 157 158
  159 167 170 171   0   0   0   0   0   0   0   0   0   0]
 [  7  11  15  17  18  19  22  24  25  27  29  35  36  37  41  48  64  66
   70  80  81  82  91  92  93  94 103 112 118 119 122 127 130 133 136 140
  141 144 145 146 148 149 153 154 157 158 159 167 170 171]]
3.3 The Features index for 32k:

[[119 127 149 159 171   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  8  24  35  67 119 122 127 149 159 171   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  6   8  24  25  29  35  67 110 112 118 119 122 127 130 140 149 153 159
  170 171   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  3   6   8  10  15  17  23  24  25  26  28  29  35  67  70 110 112 118
  119 122 127 130 133 140 149 153 159 167 170 171   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  3   6   8  10  15  17  18  19  23  24  25  26  27  28  29  35  61  67
   70  82  93 110 112 118 119 122 127 130 133 136 140 145 149 153 155 159
  165 167 170 171   0   0   0   0   0   0   0   0   0   0]
 [  3   6   8  10  11  15  16  17  18  19  23  24  25  26  27  28  29  35
   57  61  67  70  82  93 110 112 117 118 119 122 127 130 131 133 136 140
  142 145 149 153 154 155 157 159 162 165 167 169 170 171]]
3.3 The Features PP for 1k:

[[2.91724577e-05 1.28243558e-04 3.43025951e-05 2.97895967e-05
  5.02165938e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [8.45580708e-04 2.78082718e-04 8.26766444e-04 2.91724577e-05
  2.31717847e-04 1.34070183e-04 1.28243558e-04 3.43025951e-05
  2.97895967e-05 5.02165938e-06 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.85184051e-03 8.45580708e-04 1.97500733e-03 1.61158566e-03
  2.78082718e-04 8.26766444e-04 2.91724577e-05 8.56255599e-04
  2.31717847e-04 1.34070183e-04 8.16790467e-03 3.60391502e-03
  1.28243558e-04 2.04093060e-03 3.81705022e-03 3.43025951e-05
  2.17996653e-03 2.97895967e-05 2.62218990e-03 5.02165938e-06
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.85184051e-03 8.45580708e-04 2.29841657e-02 1.62737954e-02
  1.97500733e-03 2.39455355e-02 1.61158566e-03 2.78082718e-04
  2.48342588e-02 8.26766444e-04 1.48328018e-02 1.67123091e-02
  2.91724577e-05 8.56255599e-04 2.31717847e-04 1.34070183e-04
  8.16790467e-03 8.74589018e-03 3.60391502e-03 2.43137679e-02
  1.28243558e-04 9.43147470e-03 2.04093060e-03 3.81705022e-03
  3.43025951e-05 2.17996653e-03 8.51085015e-03 2.97895967e-05
  2.62218990e-03 5.02165938e-06 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [1.85184051e-03 8.45580708e-04 2.29841657e-02 1.62737954e-02
  4.50058518e-02 3.85554808e-02 4.72934009e-02 1.97500733e-03
  2.39455355e-02 4.43723072e-02 3.28396976e-02 1.61158566e-03
  2.78082718e-04 2.48342588e-02 8.26766444e-04 1.48328018e-02
  1.67123091e-02 2.91724577e-05 8.56255599e-04 2.31717847e-04
  1.34070183e-04 8.16790467e-03 8.74589018e-03 4.36655672e-02
  3.60391502e-03 2.43137679e-02 1.28243558e-04 4.23920906e-02
  9.43147470e-03 2.04093060e-03 3.81705022e-03 3.43025951e-05
  2.17996653e-03 8.51085015e-03 2.72871650e-02 3.68597108e-02
  2.97895967e-05 5.79500093e-02 2.62218990e-03 5.02165938e-06
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [6.11855806e-02 1.85184051e-03 8.45580708e-04 2.29841657e-02
  1.62737954e-02 4.50058518e-02 3.85554808e-02 5.96345748e-02
  8.09194526e-02 4.72934009e-02 1.97500733e-03 2.39455355e-02
  4.43723072e-02 8.77985267e-02 3.28396976e-02 1.61158566e-03
  2.78082718e-04 7.73417422e-02 7.03224317e-02 8.83667347e-02
  2.48342588e-02 8.26766444e-04 6.23179341e-02 1.48328018e-02
  1.67123091e-02 8.97106752e-02 2.91724577e-05 8.56255599e-04
  2.31717847e-04 1.34070183e-04 8.16790467e-03 8.74589018e-03
  4.36655672e-02 3.60391502e-03 2.43137679e-02 1.28243558e-04
  4.23920906e-02 9.43147470e-03 8.79671069e-02 2.04093060e-03
  3.81705022e-03 3.43025951e-05 2.17996653e-03 8.51085015e-03
  2.72871650e-02 3.68597108e-02 2.97895967e-05 5.79500093e-02
  2.62218990e-03 5.02165938e-06]]
3.3 The Features PP for 32k:

[[9.06768952e-130 1.45844690e-089 1.54987189e-133 4.19994083e-108
  3.45645330e-112 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000]
 [1.44098780e-074 1.26485822e-075 2.49981976e-079 2.72276497e-085
  9.06768952e-130 1.41412049e-075 1.45844690e-089 1.54987189e-133
  4.19994083e-108 3.45645330e-112 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000]
 [5.63745162e-070 1.44098780e-074 1.26485822e-075 2.31782240e-069
  1.84434934e-072 2.49981976e-079 2.72276497e-085 1.95051709e-074
  1.48220589e-061 1.53867569e-064 9.06768952e-130 1.41412049e-075
  1.45844690e-089 1.39159545e-073 1.92769034e-069 1.54987189e-133
  7.33341043e-069 4.19994083e-108 5.59133783e-070 3.45645330e-112
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000]
 [1.01090694e-052 5.63745162e-070 1.44098780e-074 3.87362637e-054
  1.69078512e-060 3.73949839e-058 7.24547226e-057 1.26485822e-075
  2.31782240e-069 4.08251893e-052 2.74446101e-050 1.84434934e-072
  2.49981976e-079 2.72276497e-085 1.22566176e-058 1.95051709e-074
  1.48220589e-061 1.53867569e-064 9.06768952e-130 1.41412049e-075
  1.45844690e-089 1.39159545e-073 4.43625223e-050 1.92769034e-069
  1.54987189e-133 7.33341043e-069 4.19994083e-108 4.23989514e-060
  5.59133783e-070 3.45645330e-112 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000]
 [1.01090694e-052 5.63745162e-070 1.44098780e-074 3.87362637e-054
  1.69078512e-060 3.73949839e-058 2.25795903e-038 9.01707706e-047
  7.24547226e-057 1.26485822e-075 2.31782240e-069 4.08251893e-052
  3.18138169e-046 2.74446101e-050 1.84434934e-072 2.49981976e-079
  4.95140872e-046 2.72276497e-085 1.22566176e-058 9.80943293e-042
  2.63302775e-044 1.95051709e-074 1.48220589e-061 1.53867569e-064
  9.06768952e-130 1.41412049e-075 1.45844690e-089 1.39159545e-073
  4.43625223e-050 3.81642592e-041 1.92769034e-069 9.57378382e-040
  1.54987189e-133 7.33341043e-069 7.15635582e-040 4.19994083e-108
  2.65973377e-047 4.23989514e-060 5.59133783e-070 3.45645330e-112
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
  0.00000000e+000 0.00000000e+000]
 [1.01090694e-052 5.63745162e-070 1.44098780e-074 3.87362637e-054
  2.97538449e-034 1.69078512e-060 2.44285042e-030 3.73949839e-058
  2.25795903e-038 9.01707706e-047 7.24547226e-057 1.26485822e-075
  2.31782240e-069 4.08251893e-052 3.18138169e-046 2.74446101e-050
  1.84434934e-072 2.49981976e-079 1.20735016e-028 4.95140872e-046
  2.72276497e-085 1.22566176e-058 9.80943293e-042 2.63302775e-044
  1.95051709e-074 1.48220589e-061 2.41831552e-029 1.53867569e-064
  9.06768952e-130 1.41412049e-075 1.45844690e-089 1.39159545e-073
  1.81417974e-030 4.43625223e-050 3.81642592e-041 1.92769034e-069
  1.01457204e-034 9.57378382e-040 1.54987189e-133 7.33341043e-069
  1.44040075e-036 7.15635582e-040 1.23324888e-037 4.19994083e-108
  4.75727551e-031 2.65973377e-047 4.23989514e-060 3.62534167e-031
  5.59133783e-070 3.45645330e-112]]
currently, 3.1 part2
currently, the best classifier would be 5.
currently, Part 3.4, Fold number 0 in 0-4, classification 1 in 1-5
currently, Part 3.4, Fold number 0 in 0-4, classification 2 in 1-5
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminatd early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
currently, Part 3.4, Fold number 0 in 0-4, classification 3 in 1-5
currently, Part 3.4, Fold number 0 in 0-4, classification 4 in 1-5
currently, Part 3.4, Fold number 0 in 0-4, classification 5 in 1-5
currently, Part 3.4, Fold number 1 in 0-4, classification 1 in 1-5
currently, Part 3.4, Fold number 1 in 0-4, classification 2 in 1-5
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminatd early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
currently, Part 3.4, Fold number 1 in 0-4, classification 3 in 1-5
currently, Part 3.4, Fold number 1 in 0-4, classification 4 in 1-5
currently, Part 3.4, Fold number 1 in 0-4, classification 5 in 1-5
currently, Part 3.4, Fold number 2 in 0-4, classification 1 in 1-5
currently, Part 3.4, Fold number 2 in 0-4, classification 2 in 1-5
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminatd early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
currently, Part 3.4, Fold number 2 in 0-4, classification 3 in 1-5
currently, Part 3.4, Fold number 2 in 0-4, classification 4 in 1-5
currently, Part 3.4, Fold number 2 in 0-4, classification 5 in 1-5
currently, Part 3.4, Fold number 3 in 0-4, classification 1 in 1-5
currently, Part 3.4, Fold number 3 in 0-4, classification 2 in 1-5
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminatd early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
currently, Part 3.4, Fold number 3 in 0-4, classification 3 in 1-5
currently, Part 3.4, Fold number 3 in 0-4, classification 4 in 1-5
currently, Part 3.4, Fold number 3 in 0-4, classification 5 in 1-5
currently, Part 3.4, Fold number 4 in 0-4, classification 1 in 1-5
currently, Part 3.4, Fold number 4 in 0-4, classification 2 in 1-5
/local/packages/python-3.7/lib/python3.7/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminatd early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)
currently, Part 3.4, Fold number 4 in 0-4, classification 3 in 1-5
currently, Part 3.4, Fold number 4 in 0-4, classification 4 in 1-5
currently, Part 3.4, Fold number 4 in 0-4, classification 5 in 1-5
TODO Section 3.4
Classifier done.
